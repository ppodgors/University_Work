---
title: "Theoretical Foundations of Large Data Sets"
subtitle: "List 3"
author: "Paulina Podgórska"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

# 1. Estimation of the Type I Error Probability

In this analysis we will estimate the Type I error probability for the
modified Higher Criticism Test, $HC_{mod}$, using the asymptotic
critical value for $0.05$ significance test, denoted as
$c_{crit} = 4.14$. We consider sample sizes $n \in \{ 5000,50000\}$. The
$HC_{mod}$ statistics is given by the following formula:
$$HC_{mod}=\underset{0<t<1}{max}\sqrt{n}\frac{F_n(t)-t}{\sqrt{t(1-t)q(t)}},$$
where $$q(t) = log log\frac{1}{t(1-t)},$$
$$F_n(t) = \frac{\sum_iV_i}{n},$$
$$V_i(t) = \mathbf{1}_{\{ p_i \leq t\}}.$$

```{r eval=FALSE, include=FALSE}
#wszystkie dane wygenerowałam wcześniej i w tabelach umieściłam te wyniki
n = c(5000, 50000)
alpha = 0.05
crit = 4.14
prob = c()
#t = 0.4
  
for(j in 1:2){
  result = c()
  t = seq(from=0.01, to=0.99, by=0.001)
  for(k in 1:1000){
    p = runif(n[j])
    HCm = c()
    for(i in 1:length(t)){
      Fn = sum(p <= t[i])/n[j]
      q = log(log(1/(t[i]*(1-t[i])) ))
      HCm = c(HCm, sqrt(n[j])*(Fn-t[i])/(sqrt(t[i]*(1-t[i])*q)))
  }
    result = c(result, max(HCm)>crit)
    cat(result[length(result)],"\n")
    }
  prob[j]=mean(result)}
```

The estimated probabilities were calculated based on 1000 iterations and
are presented in the table below:

| n = 5000 | n = 50000 |
|----------|-----------|
| 0.04     | 0.052     |

We observe that the sample size $n$ directly influences the probability
of the type I error. For $n = 50000$ we see that the probability is very
close to our significance level. This indicates that the modified Higher
Criticism Test gains precision and accuracy with increased sample size.

# 2. Estimation of the critical values

Now, let us estimate critical values of both Higher-Criticism tests at
the significance level $\alpha = 0.05$. The Higher Criticism Test
statistic is given by:
$$HC^* = \underset{\frac{1}{n}<t<\frac{1}{2}}{max}\sqrt{n}\frac{F_n(t)-t}{\sqrt{t(1-t)}}.$$

```{r eval=FALSE, include=FALSE}
n = 5000
t1 = seq(from=0.01, to=0.99, by=0.001)
t2 = seq(1/n+0.001, 1/2-0.001, by=0.001)
HCs2 = c()
HCm2 = c()
for(k in 1:1000){
  p = runif(n)
  HCs = c()
  HCm = c()
  for(ti in t2){
    Fn = sum(p <= ti)/n
    q = log(log(1/(ti*(1-ti)) ))
    HCm = c(HCm, sqrt(n)*(Fn-ti)/(sqrt(ti*(1-ti)*q)))
  }
  for(ti in t1){
    Fn = sum(p <= ti)/n
    HCs = c(HCs, sqrt(n)*(Fn-ti)/sqrt(ti*(1-ti)))
  }
  HCs2= c(HCs2, max(HCs))
  HCm2 = c(HCm2, max(HCm))
}
df3 = data.frame(x1 = round(quantile(HCs2,0.95),3), x2 = round(quantile(HCm2,0.95),3))
colnames(df3) = c("$HC^*$","$HC_{mod}$")
rownames(df3) = c("crit. value")
df3
```

The estimated critical values were calculated based on 1000 iterations
and are presented in the table below:

| $HC^*$ | $HC_{mod}$ |
|--------|------------|
| 2.997  | 3.806      |

We can observe a higher critical value for modified version of $HC$ -
it's value is closer to the critical value from task 1. This means that
the $HC_{mod}$ test is more conservative in declaring significance than
$HC^*$. This suggest that $HC_{mod}$ might provide better control over
the Type I error. This test might be beneficial in situations where it
is important to avoid false positives.

# 3. Power of different tests

In this analysis, we will compare the power of the following tests:
Higher Criticism, modified Higher Criticism, Bonferroni, chi-square,
Fisher, Kolmogorov-Smirnov and Anderson-Darling in three scenarios:

1.  $\mu_1 =1.2\sqrt{2log n},\mu_2 = ... = \mu_n = 0$,

2.  $\mu_1 = ... = \mu_{100} = 1.02 \sqrt{2log\left( \frac{n}{200}\right)},\mu_{101}=...=\mu_n = 0$,

3.  $\mu_1 = ... = \mu_{1000}=1.002 \sqrt{2log\left( \frac{n}{2000}\right)},\mu_{1001}=...=\mu_n=0$,

for $n = 5000$.

```{r eval=FALSE, include=FALSE}
library(goftest)
n = 5000
pwr_hcmod = matrix(0, nrow=1000, ncol=3)
pwr_hc = matrix(0, nrow=1000, ncol=3)
pwr_bonf = matrix(0, nrow=1000, ncol=3)
pwr_chi = matrix(0, nrow=1000, ncol=3)
pwr_fisher = matrix(0, nrow=1000, ncol=3)
pwr_ks = matrix(0, nrow=1000, ncol=3)
pwr_ad = matrix(0, nrow=1000, ncol=3)
for(i in 1:1000){
  x1 = c(rnorm(1, 1.2*sqrt(2*log(n)), 1), rnorm(n-1, 0, 1))
  x2 = c(rnorm(100, 1.02*sqrt(2*log(n/200)), 1), rnorm(n-100, 0, 1))
  x3 = c(rnorm(1000, 1.002*sqrt(2*log(n/2000)), 1), rnorm(n-1000, 0, 1))
  x = list(x1,x2,x3)
  for(j in 1:3){
    t1 = seq(from=0.01, to=0.99, by=0.001)
    t2 = seq(1/n+0.001, 1/2-0.001, by=0.001)
    p = 2*(1-pnorm(abs(x[[j]]), 0 , 1))
    HCs = c()
    HCm = c()
    for(ti in t2){
      Fn = sum(p <= ti)/n
      q = log(log(1/(ti*(1-ti)) ))
      HCm = c(HCm, sqrt(n)*(Fn-ti)/(sqrt(ti*(1-ti)*q)))
    }
    for(ti in t1){
      Fn = sum(p <= ti)/n
      HCs = c(HCs, sqrt(n)*(Fn-ti)/sqrt(ti*(1-ti)))
    }
    pwr_hc[i,j] = max(HCs)>2.997
    
    pwr_hcmod[i,j] = max(HCm)>4.14
    
    pwr_bonf[i,j] = (min(p)<=0.05/n)
    
    pwr_chi[i,j] = (sum(x[[j]]^2) > qchisq(1-0.05, n))
    
    pwr_fisher[i,j] = -2*sum(log(p))>qchisq(1-0.05, 2*n)
    
    pwr_ks[i,j] = (ks.test(x[[j]], "pnorm")$p<0.05)
    
    pwr_ad[i,j] = (ad.test(x[[j]], null="pnorm")$p<0.05)
  }
  
}
df31 = data.frame("HC" = colMeans(pwr_hc),"HC_m" = colMeans(pwr_hcmod),"Bonf" = colMeans(pwr_bonf),"chi" = colMeans(pwr_chi),"fisher" = colMeans(pwr_fisher),"KS" = colMeans(pwr_ks),"ad" = colMeans(pwr_ad))

```

```{r echo=FALSE}
library(knitr)
df31 = data.frame("HC" = c(0.053, 1.000, 1.000),"HC_m" = c(0.027, 0.999, 1.000),"Bonf" = c(0.734, 0.978, 0.677),"chi" =c(0.086, 1.000, 1.000),"fisher" = c(0.073, 1.000, 1.000),"KS" = c(0.056, 0.684, 1.000),"ad" = c(0.067, 0.997, 1.000))
colnames(df31) = c("$HC^*$","$HC_{mod}$","Bonferonni","chi-square", "Fisher","K-S","A-D")
rownames(df31) = c("1.","2.","3.")
kable(df31)
```

In the case of the needle in haystack problem (1), we can see that, as
discussed in the lecture, Bonferroni's method is a clear winner. Its
power is higher than 0.7, which is at least 7 times higher than for any
other test. In the case of many small effects with the larger size of
the needle (2), we observe that all tests did significantly better. But
we have 4 top-runners with the maximum power - $HC^*$, $HC_{mod}$,
chi-square and Fisher. Worst performer is the Kolmogorov-Smirnov test
with power under 0.7. In the 3rd case, almost all test performed
excellently, except the Bonferroni test, which we know performs worst in
the case where the needle is small.

# 4. Sparse mixture model

Let us consider the sparse mixture model
$$f(\mu) = (1-\epsilon)\delta_0+\epsilon\delta_\mu$$ with
$\epsilon = n^{-\beta}$ and $\mu=\sqrt{2rlogn}$.\

We will perform analysis for each of the settings
$\beta = \{0.6,0.8 \}$, $r = \{0.1,0.4 \}$ and $n = \{5000,50000 \}$.

## Critical values for the Neyman - Pearson test

We will simulate the critical values for the Neyman-Pearson test in the
sparse mixture. The likelihood ratio for this problem is given by
$$L =  \prod_{i=1}^{n} \left((1-\epsilon)+\epsilon \exp\{\mu X_i-\frac {\mu^2}{2} \} \right).$$

```{r eval=FALSE, include=FALSE}
rep = 1000
n = c(5000,50000)
beta = c(0.6,0.8)
r = c(0.1, 0.4)
n = c(5000,50000)
### n - 5000 ###
crit1 = matrix(0, nrow = rep, ncol = 4)
# r1b1|r1b2| r2b1|r2b2
for(i in 1:rep){
  count = 1
  for(r1 in 1:2){
    for(b1 in 1:2){
      eps = n[1]^(-beta[b1])
      mi = sqrt(2*r[r1]*log(n[1]))
      X1 = rnorm(n[1])
      L = prod((1-eps)+eps*exp(mi*X1-(mi^2)/2))
      crit1[i, count] =L
      count = count + 1
    }
  }
}
df1 = data.frame(crit1)
df1 = t(df1)
colnames(df1) = c("r1beta1", "r1beta2","r2,beta1", "r2beta2")
rownames(df1) = c("crit_val")

crit_vals = c()
for(i in 1:4){
  crit_vals[i] = quantile(df1[,i],0.95)
}
### n = 50000 ###
crit1 = matrix(0, nrow = rep, ncol = 4)
# r1b1|r1b2| r2b1|r2b2
for(i in 1:rep){
  count = 1
  for(r1 in 1:2){
    for(b1 in 1:2){
      eps = n[2]^(-beta[b1])
      mi = sqrt(2*r[r1]*log(n[2]))
      X1 = rnorm(n[2])
      L = prod((1-eps)+eps*exp(mi*X1-(mi^2)/2))
      crit1[i, count] =L
      count = count + 1
    }
  }
}
df1 = data.frame(crit1)
df1 = t(df1)
colnames(df1) = c("r1beta1", "r1beta2","r2,beta1", "r2beta2")
rownames(df1) = c("crit_val")

crit_vals = c()
for(i in 1:4){
  crit_vals[i] = quantile(df1[,i],0.95)
}
```

```{r echo=FALSE}
df5000 = data.frame(t(c(2.937129, 1.301748, 0.2061482, 2.550398)))
df50000 = data.frame(t(c(2.959070335, 1.190324396, 0.000765595, 2.614661781)))
colnames(df5000) = c("$r_1,\\beta_1$", "$r_1,\\beta_2$","$r_2,\\beta_1$", "$r_2,\\beta_2$")
rownames(df5000) = c("crit. val.")

colnames(df50000) = c("$r_1,\\beta_1$", "$r_1,\\beta_2$","$r_2,\\beta_1$", "$r_2,\\beta_2$")
rownames(df50000) = c("crit. val.")
kable(df5000, caption = "n = 5000")
kable(df50000, caption = "n = 50000")
```

We observe that our test becomes more conservative (for both n) when
$r$ and $\beta$ are smaller; in this cases, the critical value nearly reaches
3. Also, when both values are larger, the estimated critical
value for test is second highest. The impact of $n$ on the critical
value is most noticeable when $r = 0.8$ and $\beta = 0.1$. The critical value for the larger $n$ is over 200 times
higher than for $n=5000$. This indicates a strong influence of the combination of $r$ and $\beta$ on the critical value, however, the precise nature of this relationship is not immediately apparent.

## Comparison of power of different tests

Now, let us compare the power of the Neyman-Pearson test to the power of
both versions of HC, Bonferroni, Fisher and chi-square.

```{r eval=FALSE, include=FALSE}
pwr_NP = matrix(0, nrow = rep, ncol = 4)
pwr_HC = matrix(0, nrow = rep, ncol = 4)
pwr_HCm = matrix(0, nrow = rep, ncol = 4)
pwr_bon = matrix(0, nrow = rep, ncol = 4)
pwr_fisher = matrix(0, nrow = rep, ncol = 4)
pwr_chi = matrix(0, nrow = rep, ncol = 4)
t1 = seq(from=0.01, to=0.99, by=0.001)
t2 = seq(1/n[1]+0.001, 1/2-0.001, by=0.001)
# r1b1|r1b2| r2b1|r2b2
for(i in 1:rep){
  count = 1
  for(r1 in 1:2){
    for(b1 in 1:2){
      eps = n[1]^(-beta[b1])
      mi = sqrt(2*r[r1]*log(n[1]))
      X1 = sample(1:0, n[1], prob=c(1-eps,eps), replace = TRUE)
      X1 = ifelse(X1 == 1, rnorm(sum(X1 == 1), 0, 1), rnorm(sum(X1 == 0), mi, 1))
      p = 2*(1-pnorm(abs(X1), 0 , 1))
          HCs = c()
          HCm = c()
          for(ti in t2){
            Fn = sum(p <= ti)/n[1]
            q = log(log(1/(ti*(1-ti)) ))
            HCm = c(HCm, sqrt(n[1])*(Fn-ti)/(sqrt(ti*(1-ti)*q)))
          }
          for(ti in t1){
            Fn = sum(p <= ti)/n[1]
            HCs = c(HCs, sqrt(n[1])*(Fn-ti)/sqrt(ti*(1-ti)))
          }
      pwr_HC[i,count] = max(HCs)>2.997
          
      pwr_HCm[i,count] = max(HCm)>4.14
      L = prod((1-eps)+eps*exp(mi*X1-(mi^2)/2))
      
      pwr_NP[i,count] = L>df5000[count]
      
      pwr_bon[i,count] = (min(p)<=0.05/n[1])
      
      pwr_chi[i,count] = sum(X1^2)>qchisq(1-0.05, n[1])
      
      pwr_fisher[i,count] = -2*sum(log(p))>qchisq(1-0.05, 2*n[1])
      count = count + 1
    }
  }
}
df41 = data.frame("NP" = colMeans(pwr_NP),HC = colMeans(pwr_HC),"HCmod" = colMeans(pwr_HCm),"bon" = colMeans(pwr_bon),"fisher" = colMeans(pwr_fisher),"chi" = colMeans(pwr_chi))
df41
```

```{r echo=FALSE}
df41p = data.frame("NP" = c(0.215, 0.073, 0.971, 0.335),HC = c(0.100, 0.062, 0.453, 0.081),"HCmod" = c(0.048, 0.036, 0.231, 0.039),"bon" = c(0.067, 0.054, 0.538, 0.176),"fisher" = c(0.116, 0.071, 0.557, 0.097),"chi" = c(0.127, 0.067, 0.643, 0.110))
colnames(df41p) = c("N-P","$HC^*$","$HC_{mod}$","Bonf.","Fisher","chi-sq")
rownames(df41p) = c("$r_1,\\beta_1$", "$r_1,\\beta_2$","$r_2,\\beta_1$", "$r_2,\\beta_2$")
kable(df41p, caption="n = 5000")
```

```{r eval=FALSE, include=FALSE}
pwr_NP = matrix(0, nrow = rep, ncol = 4)
pwr_HC = matrix(0, nrow = rep, ncol = 4)
pwr_HCm = matrix(0, nrow = rep, ncol = 4)
pwr_bon = matrix(0, nrow = rep, ncol = 4)
pwr_fisher = matrix(0, nrow = rep, ncol = 4)
pwr_chi = matrix(0, nrow = rep, ncol = 4)
t1 = seq(from=0.01, to=0.99, by=0.001)
t2 = seq(1/n[2]+0.001, 1/2-0.001, by=0.001)
# r1b1|r1b2| r2b1|r2b2
for(i in 1:rep){
  count = 1
  for(r1 in 1:2){
    for(b1 in 1:2){
      eps = n[2]^(-beta[b1])
      mi = sqrt(2*r[r1]*log(n[2]))
      X1 = sample(1:0, n[2], prob=c(1-eps,eps), replace = TRUE)
      X1 = ifelse(X1 == 1, rnorm(sum(X1 == 1), 0, 1), rnorm(sum(X1 == 0), mi, 1))
      p = 2*(1-pnorm(abs(X1), 0 , 1))
          HCs = c()
          HCm = c()
          for(ti in t2){
            Fn = sum(p <= ti)/n[2]
            q = log(log(1/(ti*(1-ti)) ))
            HCm = c(HCm, sqrt(n[2])*(Fn-ti)/(sqrt(ti*(1-ti)*q)))
          }
          for(ti in t1){
            Fn = sum(p <= ti)/n[2]
            HCs = c(HCs, sqrt(n[2])*(Fn-ti)/sqrt(ti*(1-ti)))
          }
      pwr_HC[i,count] = max(HCs)>2.997
          
      pwr_HCm[i,count] = max(HCm)>4.14
      L = prod((1-eps)+eps*exp(mi*X1-(mi^2)/2))
      
      pwr_NP[i,count] = L>df5000[count]
      
      pwr_bon[i,count] = (min(p)<=0.05/n[2])
      
      pwr_chi[i,count] = sum(X1^2)>qchisq(1-0.05, n[2])
      
      pwr_fisher[i,count] = -2*sum(log(p))>qchisq(1-0.05, 2*n[2])
      count = count + 1
    }
  }
}
df42 = data.frame("NP" = colMeans(pwr_NP),HC = colMeans(pwr_HC),"HCmod" = colMeans(pwr_HCm),"bon" = colMeans(pwr_bon),"fisher" = colMeans(pwr_fisher),"chi" = colMeans(pwr_chi))
```

```{r echo=FALSE}
df42p = data.frame("NP" = c(0.234, 0.014, 0.996, 0.378),HC = c(0.087, 0.049, 0.386, 0.069),"HCmod!" = c(0.053, 0.038, 0.162, 0.027),"bon!" = c(0.070, 0.054, 0.693, 0.183),"fisher" = c(0.117, 0.061, 0.533, 0.065),"chi" = c(0.127, 0.062, 0.623, 0.066))
colnames(df42p) = c("N-P","$HC^*$","$HC_{mod}$","Bonf.","Fisher","chi-sq")
rownames(df42p) = c("$r_1,\\beta_1$", "$r_1,\\beta_2$","$r_2,\\beta_1$", "$r_2,\\beta_2$")
kable(df42p, caption="n = 50000")
```
Firstly, let us analyze the influence of sample size on the power. It seems that the tests most affected by changes in $n$ are the HC tests - for these, we can notice a significant increase in power for larger sample sizes. On the other hand, we cannot determine a strong influence of $n$ on the performance of the Neyman-Pearson, Bonferroni, Fisher and chi-square tests. Overall, the test that almost consistently performs the best, regardless of the value of $r$ and $\beta$ is the Neyman-Pearson test. It is the only test that achieves the power very close to one for $r = 0.4$ and $\beta = 0.6$. We also observe that $HC_{mod}$ achieves the worst results for all 8 scenarios. 

From the theory learned in class, we know that the power of the test is
depended on the threshold effect. There is a threshold curve for $r$ of the
form $$\rho^*(\beta) = \left\{\begin{matrix}
\beta - \frac{1}{2},  \hspace{3cm} \frac{1}{2}<\beta\leq \frac{3}{4}\\ (1-\sqrt{1-\beta})^2, \hspace{1.4cm} \frac{3}{4}\leq \beta \leq 1
\end{matrix} ,\right.$$
such that if $r>\rho^*(\beta)$ we can the N-P test to achieve
$$\mathcal{P}_{0}( Type\ I\ Error) +\mathcal{P}_{1}( Type\ II\ Error) \longrightarrow 0$$
and if $r<\rho^*(\beta)$, then for any test
$$lim\ inf\ \mathcal{P}_{0}( Type\ I\ Error) +\mathcal{P}_{1}( Type\ II\ Error) \geqslant 1.$$
Let us calculate the threshold for our parameters:

1.   $r = 0.1$

-    $\beta = 0.6 \rightarrow \rho^*(\beta) =$ `r 0.6 - 1/2` 

-    $\beta = 0.8 \rightarrow \rho^*(\beta) =$ `r round((1-sqrt(1-0.8))^2,2)` 

We can see that for $\beta = 0.6$, the threshold is equal to $r$. We can notice better performance in our tests in this case in contrast to $\beta_2$. As expected, for all tests in the second scenario, the power is very low - below 0.1.

2.   $r = 0.4$

-    $\beta = 0.6 \rightarrow \rho^*(\beta) =$ `r 0.6 - 1/2` 

-    $\beta = 0.8 \rightarrow \rho^*(\beta) =$ `r round((1-sqrt(1-0.8))^2,2)` 

We can observe, that when $r$ is a lot bigger than the threshold, our tests perform the best. The clear winner is the Neyman - Person test. 